{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979923b5-0493-4768-ad1a-06db54f0bc7a",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "DS 5001 Exploratory Text Analytics | Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f57f-12ed-4259-be3d-60cb67b8d044",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "- Full Name: Ryan Lipps\n",
    "- Userid: rhl8pk\n",
    "- GitHub Repo URL: https://github.com/rlipps/Exploratory_Text_Analytics_Final\n",
    "- UVA Box URL: https://virginia.app.box.com/folder/262013200623"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd11d-eb04-4bcc-b115-f205f367de49",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of the final project is for you to create a **digital analytical edition** of a corpus using the tools, practices, and perspectives you’ve learning in this course. You will select a corpus that has already been digitized and transcribed, parse that into an F-compliant set of tables, and then generate and visualize the results of a series of fitted models. You will also draw some tentative conclusions regarding the linguistic, cultural, psychological, or historical features represented by your corpus. The point of the exercise is to have you work with a corpus through the entire pipeline from ingestion to interpretation. \n",
    "\n",
    "Specifically, you will acquire a collection of long-form texts and perform the following operations:\n",
    "\n",
    "- **Convert** the collection from their source formats (F0) into a set of tables that conform to the Standard Text Analytic Data Model (F2).\n",
    "- **Annotate** these tables with statistical and linguistic features using NLP libraries such as NLTK (F3).\n",
    "- **Produce** a vector representation of the corpus to generate TFIDF values to add to the TOKEN (aka CORPUS) and VOCAB tables (F4).\n",
    "- **Model** the annotated and vectorized model with tables and features derived from the application of unsupervised methods, including PCA, LDA, and word2vec (F5).\n",
    "- **Explore** your results using statistical and visual methods.\n",
    "- **Present** conclusions about patterns observed in the corpus by means of these operations.\n",
    "\n",
    "When you are finished, you will make the results of your work available in GitHub (for code) and UVA Box (for data). You will submit to Gradescope (via Canvas) a PDF version of a Jupyter notebook that contains the information listed below.\n",
    "\n",
    "# Some Details\n",
    "\n",
    "- Please fill out your answers in each task below by editing the markdown cell. \n",
    "- Replace text that asks you to insert something with the thing, i.e. replace `(INSERT IMAGE HERE)` with an image element, e.g. `![](image.png)`.\n",
    "- For URLs, just paste the raw URL directly into the text area. Don't worry about providing link labels using `[label](link)`.\n",
    "- Please do not alter the structure of the document or cell, i.e. the bulleted lists. \n",
    "- You may add explanatory paragraphs below the bulleted lists.\n",
    "- Please name your tables as they are named in each task below.\n",
    "- Tasks are indicated by headers with point values in parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b6d68-e039-4612-858b-29510eeb5365",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0889de-cd53-4aa5-80b2-a2a39060776a",
   "metadata": {},
   "source": [
    "## Source Description (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e395a-4b0b-4ba3-9112-80c733998dbe",
   "metadata": {},
   "source": [
    "Provide a brief description of your source material, including its provenance and content. Tell us where you found it and what kind of content it contains.\n",
    "\n",
    "The source material is a collection of some of my favorite musicians' albums. It consists of 953 songs from 89 albums from 18 artists. The data is from both the Spotify API and lyricsgenius API. I used the Spotify API to get all of the artist, album, and song metadata, and used this information to query the lyricsgenius API to get the lyrics. It involved heavy cleaning, as the code to get the lyrics was automated as initially the full song list consisted of over 1000 songs. There were some plays, song lists, and many other interesting things that the API returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b507c1-6dc2-44f7-b74c-790d84a48e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Features (1)\n",
    "\n",
    "Add values for the following items. (Do this for all following bulleted lists.)\n",
    "\n",
    "- Source URL: https://developer.spotify.com/documentation/web-api and https://docs.genius.com/\n",
    "- UVA Box URL: https://virginia.box.com/s/66ye5cvhcpazvqi19qpbzwfqqibk8scf\n",
    "- Number of raw documents: 89\n",
    "- Total size of raw documents (e.g. in MB): 1.2 MB\n",
    "- File format(s), e.g. XML, plaintext, etc.: .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e81b1-9f70-47b5-bb25-49be4e76b98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Document Structure (1)\n",
    "\n",
    "Provide a brief description of the internal structure of each document. That, describe the typical elements found in document and their relation to each other. For example, a corpus of letters might be described as having a date, an addressee, a salutation, a set of content paragraphs, and closing. If they are various structures, state that.\n",
    "\n",
    "Each document is an album, with lyrics for each song, in the order of the tracklist. If a song is instrumental it was skipped. For the most part the genius API returns lyrics following some structural information such as [VERSE 1]...[PRE-CHORUS 1]...[CHORUS 1], though this is not consistent. Overall for this project, bags were either ALBUM or SONG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4c9f-e101-46fe-ac59-a35a1b148a4b",
   "metadata": {},
   "source": [
    "# Parsed and Annotated Data\n",
    "\n",
    "Parse the raw data into the three core tables of your addition: the `LIB`, `CORPUS`, and `VOCAB` tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows.\n",
    "\n",
    "You may consider using `|` as a delimitter.\n",
    "\n",
    "Provide the following information for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d05ce4-ac5c-43ea-a07b-c4626338f80e",
   "metadata": {},
   "source": [
    "## LIB (2)\n",
    "\n",
    "The source documents the corpus comprises. These may be books, plays, newspaper articles, abstracts, blog posts, etc. \n",
    "\n",
    "Note that these are *not* documents in the sense used to describe a bag-of-words representation of a text, e.g. chapter.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/oopf095w1m89gh6h85dblfu88nnfk6qf\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Number of observations: 89\n",
    "- List of features, including at least three that may be used for model summarization (e.g. date, author, etc.): 'album_name', 'album_title', 'artist', 'source_file_path', 'song_regex',\n",
    "       'genres', 'release_date', 'label', 'mean_danceability', 'mean_energy',\n",
    "       'mean_loudness', 'mean_speechiness', 'mean_acousticness',\n",
    "       'mean_instrumentalness', 'mean_liveness', 'mean_valence', 'mean_tempo',\n",
    "       'album_term_count', 'album_character_count', 'genre'\n",
    "- Average length of each document in characters: 8749.236"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304204a5-00be-46ad-b98b-0d10a9c8ca4b",
   "metadata": {},
   "source": [
    "## CORPUS (2)\n",
    "\n",
    "The sequence of word tokens in the corpus, indexed by their location in the corpus and document structures.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/vfhwbfbga0d7xl59x5trif85gjfem2gv\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Number of observations Between (should be >= 500,000 and <= 2,000,000 observations.): 192607\n",
    "- OHCO Structure (as delimitted column names): album_id, song_num, stanza_num, line_num, token_num\n",
    "- Columns (as delimitted column names, including `token_str`, `term_str`, `pos`, and `pos_group`): ['pos_tuple', 'pos', 'token_str', 'term_str', 'pos_group']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3214e-e6dd-42d6-842f-555d0058986e",
   "metadata": {},
   "source": [
    "## VOCAB (2)\n",
    "\n",
    "The unique word types (terms) in the corpus.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/y9fzwn51o4539p4p2o5hvupuw30ujw04\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Number of observations: 9110\n",
    "- Columns (as delimitted names, including `n`, `p`', `i`, `dfidf`, `porter_stem`, `max_pos` and `max_pos_group`, `stop`): n,n_chars, p, i, max_pos, max_pos_group, stop, porter_stem, song_dfidf, album_dfidf\n",
    "- Note: Your VOCAB may contain ngrams. If so, add a feature for `ngram_length`.\n",
    "- List the top 20 significant words in the corpus by DFIDF.\n",
    "\n",
    "['him', 'bad', 'true', 'own', 'living', 'fire', 'first', 'found', 'feet',\n",
    "       'someone', 'ask', 'kind', 'play', 'took', 'everyone', 'myself', 'seen',\n",
    "       'save', 'once', 'sky']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dabdc-baae-4408-95bc-2f735824d59b",
   "metadata": {},
   "source": [
    "# Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ef9c-1cb5-41e8-a5ee-1e37428b4539",
   "metadata": {},
   "source": [
    "## BOW (3)\n",
    "\n",
    "A bag-of-words representation of the CORPUS.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/vjmoudem87ohv1h9u0c8e9161syf7cbk\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Bag (expressed in terms of OHCO levels): album_id, OHCO[:1]\n",
    "- Number of observations: 43158\n",
    "- Columns (as delimitted names, including `n`, `tfidf`): n, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29890d2f-bf96-43ad-8d08-792393830163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DTM (3)\n",
    "\n",
    "A represenation of the BOW as a sparse count matrix.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/2vj1kgimc8r31oiovr0up5jxqh02mpca\n",
    "- UVA Box URL of BOW used to generate (if applicable): https://virginia.box.com/s/vjmoudem87ohv1h9u0c8e9161syf7cbk\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Bag (expressed in terms of OHCO levels): album_id, OHCO[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b4774-7c76-401d-a9de-2704f28a0821",
   "metadata": {},
   "source": [
    "## TFIDF (3)\n",
    "\n",
    "A Document-Term matrix with TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/u07hfehp69gy8kdy6p63l6dh7tclivqx\n",
    "- UVA Box URL of DTM or BOW used to create: https://virginia.box.com/s/2vj1kgimc8r31oiovr0up5jxqh02mpca\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Description of TFIDIF formula ($\\LaTeX$ OK):  $\\frac{TF}{TF_{max}}*log_{2}\\frac{N}{DF}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f5ca-5361-4701-b9dd-9da66859b40b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduced and Normalized TFIDF_L2 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548dd2-f692-4365-936c-39c84df79b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Document-Term matrix with L2 normalized TFIDF values.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/tu3n2sgmvgbs7275229t4qwwfokuiu2d\n",
    "- UVA Box URL of source TFIDF table: https://virginia.box.com/s/u07hfehp69gy8kdy6p63l6dh7tclivqx\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Build_Tables.ipynb\n",
    "- Delimitter: |\n",
    "- Number of features (i.e. significant words): 2000\n",
    "- Principle of significant word selection: I used only singular and plural nouns and removed stopwords. The inclusion of stopwords is that there are many stopwords that were getting marked as significant because it appears that they're used as ad-libs and vocalizations in songs, which was adding a lot of noise. I used only nouns as using multiple parts of speech was very noisy and not very interprable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50da94-af36-4e8d-b1a7-24dbcf431880",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df79264-dd93-4199-be38-db31579b7ce8",
   "metadata": {},
   "source": [
    "## PCA Components (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/2unm4woxrwt99s22mjujiuvhhtiarsmj\n",
    "- UVA Box URL of the source TFIDF_L2 table: https://virginia.box.com/s/tu3n2sgmvgbs7275229t4qwwfokuiu2d\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/PCA.ipynb\n",
    "- Delimitter: |\n",
    "- Number of components: 10\n",
    "- Library used to generate: by hand, with scipy linalg\n",
    "- Top 5 positive terms for first component: mystery step hes lie everything\n",
    "- Top 5 negative terms for second component: someones denial hes round hope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc882-cbce-4d24-9923-5d36ac609f43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA DCM (4)\n",
    "\n",
    "The document-component matrix generated.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/rh8bcr8e2tjdw4yngk921c8w2gcgrrdc\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/PCA.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd2a4a-7f2f-4259-a5c4-063168cb1b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA Loadings (4)\n",
    "\n",
    "The component-term matrix generated.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/k8q2qjmq4vnjwqh3agi4ulugrqjovks0\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/PCA.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fff42f-6665-4941-ba3d-034627dc0124",
   "metadata": {},
   "source": [
    "## PCA Visualization 1 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the first two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](images/pca1.png)\n",
    "\n",
    "![](images/loadings1.png)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the first component:\n",
    "\n",
    "The polarity in the first component looks like teenage angst on the negative axis and more mature concepts on the positive axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb54565-7669-4a2f-90b2-a4c283277c02",
   "metadata": {},
   "source": [
    "## PCA Visualization 2 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the second two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "![](images/pca2.png)\n",
    "\n",
    "![](images/loadings2.png)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the second component:\n",
    "\n",
    "The polarity in the second component seems to related to general feelings and teenage angst again on the positive axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee23b2-25d1-4226-bf31-1607e5ed4677",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA TOPIC (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/1jcx53mxamzg1twz37uv9gv6l01knoj7\n",
    "- UVA Box URL of count matrix used to create: https://virginia.box.com/s/v2rq9jzsq00lleq9sndfqu9r4kpuqhz1\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/LDA.ipynb\n",
    "- Delimitter: |\n",
    "- Libary used to compute: sklearn\n",
    "- A description of any filtering, e.g. POS (Nouns and Verbs only): Nouns only\n",
    "- Number of components: 10\n",
    "- Any other parameters used: removed stopwords for previous description\n",
    "- Top 5 words and best-guess labels for topic five topics by mean document weight:\n",
    "  - T00: way time something eyes heart: this topic seems to indicate some waywardness and being lost\n",
    "  - T01: time war house love arms: this topic seems to be about conflict, maybe relating to domestic relationships\n",
    "  - T02: love life sacrilege eyes look: this topic seems to be about infidelity or lust\n",
    "  - T03: time raindrops guns duh days: this topic seems to be about malaise\n",
    "  - T04: dreamt baby love everything lie: this topic seems to be about uncertainty in relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518d520-4a5c-48fa-836d-f8ea3e3c2f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA THETA (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/5b0t68l18xxefb3g5q0u72rjwn9qtw1m\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/LDA.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8808b30-64f4-4249-95d5-d7c0925ce432",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA PHI (4)\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/qc69sz3q84euwkehr4z1hsfgdzoy68ki\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/LDA.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e404bf-8a2a-4eb4-ba89-0c708c8f359d",
   "metadata": {},
   "source": [
    "## LDA + PCA Visualization (4)\n",
    "\n",
    "Apply PCA to the PHI table and plot the topics in the space opened by the first two components.\n",
    "\n",
    "Size the points based on the mean document weight of each topic (using the THETA table).\n",
    "\n",
    "Color the points basd on a metadata feature from the LIB table.\n",
    "\n",
    "Provide a brief interpretation of what you see.\n",
    "\n",
    "![](images/lda1.png)\n",
    "\n",
    "* A few artists are not max assigned to a topic.\n",
    "* HOLYCHILD and Caroline Polachek are separated along PC1, which is interesting given they're relatively similar in genre.\n",
    "* Using hover info, max artist assignment and max album assignment don't always line up for topics\n",
    "    * Topics 1 and 12 have agreement between Big Thief and Big Thief albums, and in my opinion those two albums are definitely their most focused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1f327-a386-476a-8d94-2ab7a63afa7a",
   "metadata": {},
   "source": [
    "## Sentiment VOCAB_SENT (4)\n",
    "\n",
    "Sentiment values associated with a subset of the VOCAB from a curated sentiment lexicon.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/0816t62hj7ztuj0v6sks2aisnk32o0ou\n",
    "- UVA Box URL for source lexicon: https://virginia.box.com/s/0r5ateb6mu8fq5nw6uaabgqfslq8dslq\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Sentiment_Analysis.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9d67-1560-4be9-b82a-b99a60b5c93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment BOW_SENT (4)\n",
    "\n",
    "Sentiment values from VOCAB_SENT mapped onto BOW.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/apkph60vbm2k9tpn44bwj0cdh0ow2zr9\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Sentiment_Analysis.ipynb\n",
    "- Delimitter: |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee6837-b12e-453d-96c1-59eaa4b28883",
   "metadata": {},
   "source": [
    "## Sentiment DOC_SENT (4)\n",
    "\n",
    "Computed sentiment per bag computed from BOW_SENT.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/j9i03fymod7kse758ov0qf0uu5t43jtf\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Sentiment_Analysis.ipynb\n",
    "- Delimitter: |\n",
    "- Document bag expressed in terms of OHCO levels: album_id, OHCO[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cba13-e60a-4940-a06d-02479f002c3c",
   "metadata": {},
   "source": [
    "## Sentiment Plot (4)\n",
    "\n",
    "Plot sentiment over some metric space, such as time.\n",
    "\n",
    "If you don't have a metric metadata features, plot sentiment over a feature of your choice.\n",
    "\n",
    "You may use a bar chart or a line graph.\n",
    "\n",
    "![](images/sentiment1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2316-317b-4d95-a804-aff98242e411",
   "metadata": {},
   "source": [
    "## VOCAB_W2V (4)\n",
    "\n",
    "A table of word2vec features associated with terms in the VOCAB table.\n",
    "\n",
    "- UVA Box URL: https://virginia.box.com/s/k1337c8mhi3o9640a2v7r0j0se7ke4un\n",
    "- GitHub URL for notebook used to create: https://github.com/rlipps/Exploratory_Text_Analytics_Final/blob/main/notebooks/Word_Embeddings.ipynb\n",
    "- Delimitter: |\n",
    "- Document bag expressed in terms of OHCO levels: album_id, OHCO[:1]\n",
    "- Number of features generated: 246\n",
    "- The library used to generate the embeddings: gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c1974-047b-4285-9f4d-7f3314f39542",
   "metadata": {},
   "source": [
    "## Word2vec tSNE Plot (4)\n",
    "\n",
    "Plot word embedding featues in two-dimensions using t-SNE.\n",
    "\n",
    "Describe a cluster in the plot that captures your attention.\n",
    "\n",
    "![](images/tsne.png)\n",
    "\n",
    "Zooming in on the bottom there is a lot of emotion and relationship related words, such as: hopeless, devotion, understand, believed, trust, needed, loving, regret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75878341-7fe8-4e22-b908-36029f9818e8",
   "metadata": {},
   "source": [
    "# Riffs\n",
    "\n",
    "Provde at least three visualizations that combine the preceding model data in interesting ways.\n",
    "\n",
    "These should provide insight into how features in the LIB table are related. \n",
    "\n",
    "The nature of this relationship is left open to you -- it may be correlation, or mutual information, or something less well defined. \n",
    "\n",
    "In doing so, consider the following visualization types:\n",
    "\n",
    "- Hierarchical cluster diagrams\n",
    "- Heatmaps\n",
    "- Scatter plots\n",
    "- KDE plots\n",
    "- Dispersion plots\n",
    "- t-SNE plots\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62acf1-6bb0-45d0-aed2-863b285f8cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 1 (5)\n",
    "\n",
    "![](images/riff1.png)\n",
    "\n",
    "Generally my work indicated that I listen to a lot of sad and emotional music. I was looking to see who is the saddest artist with this. My guess is that it would be the National or Radiohead. I was surprised to see that Yeah Yeah Yeahs was as sad as they were because their music is generally upbeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155a072-02b3-4aa8-b9f1-e43a59e9a85d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 2 (5)\n",
    "\n",
    "![](images/riff2.png)\n",
    "\n",
    "For this riff I was looking to see if song sentiment was related to danceability. One might expect that happier songs are more danceable, however there doesn't appear to be a correlation. For note, this is Spotify's definition of danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067c59b-8983-4acc-972a-1ecd852ded57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 3 (5)\n",
    "\n",
    "![](images/riff3.png)\n",
    "\n",
    "![](images/riff32.png)\n",
    "\n",
    "![](images/riff33.png)\n",
    "\n",
    "Generally, albums have narrative arcs, and I was interested to see if there was any trend in sentiment by song across album. Since album narratives told over songs generally discretized into songs rather than having arcs over chapters, I looked at this by graphing sentiment by track number grouped by album for a few artists that fleshed out catalogs. It looks like albums generally bounce around more that I thought - I was expecting more distinct shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e25c6e-2624-4899-829e-e7d60c878685",
   "metadata": {},
   "source": [
    "# Interpretation (4)\n",
    "\n",
    "Describe something interesting about your corpus that you discovered during the process of completing this assignment.\n",
    "\n",
    "At a minumum, use 250 words, but you may use more. You may also add images if you'd like.\n",
    "\n",
    "I learned a lot over the course of this project. Especially building a corpus by hand took a lot of effort, attention, and cleaning, which really makes me appreciate the people that work to build and clean text datasets for popular use.\n",
    "\n",
    "One of the biggest realizations was that it was very difficult to find meaningful insights, as it seems as though a lot of these artists write about the same things. This makes sense as they’re all my favorite bands and I suppose I have a “type”, but I was expecting a little more differentiation. This highlights that even though lyrics can carry a lot of emotion, the instrumentation and delivery of the lyrics carry a significant amount of meaning. This seems trivial, but I feel as though the lack of differentiation in the lyrics analysis highlights this; each of my artists write about very similar things and use a similar vocabulary, however, their styles and musical artistic choices can be relatively disparate. With this in mind I will be doing more critical lyric-listening to see if indeed all of my favorite artists are giving more or less the same messages. Furthermore, it has inspired me to expand this project to other genres and seeing if this trend carries or if I really just like one kind of music.\n",
    "\n",
    "As I mentioned in some of the notes, I also found that using lyrics for this project is particularly difficult. Most songs are very short when you look at the lyrics written down, and it was a lot of work to curate a reasonably sized corpus. I ran into some issues where a few songs have very few lyrics, and they were showing up in some topic modeling. For instance, LCD Soundsystem has a song called “Yeah” and…you guessed it…the only lyric is “Yeah” a bunch of times. This also highlights the different use of language in songs. A lot of stopwords can be used as ad-libs and vocalizations, and there are some people that rely heavily on this. It is an interesting use-case to include these to see who is performing a lot of melisma, but it was frustrating to deal with in getting a good signal to noise ratio for the deeper insights for this work.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
